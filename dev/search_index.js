var documenterSearchIndex = {"docs":
[{"location":"","page":"Home","title":"Home","text":"CurrentModule = GCPDecompositions","category":"page"},{"location":"#GCPDecompositions:-Generalized-CP-Decompositions","page":"Home","title":"GCPDecompositions: Generalized CP Decompositions","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for GCPDecompositions.","category":"page"},{"location":"","page":"Home","title":"Home","text":"ðŸ‘‹ This package provides research code and work is ongoing. If you are interested in using it in your own research, I'd love to hear from you and collaborate! Feel free to write: hong@udel.edu","category":"page"},{"location":"","page":"Home","title":"Home","text":"Please cite the following papers for this technique:","category":"page"},{"location":"","page":"Home","title":"Home","text":"David Hong, Tamara G. Kolda, Jed A. Duersch. \"Generalized Canonical Polyadic Tensor Decomposition\", SIAM Review 62:133-163, 2020. https://doi.org/10.1137/18M1203626 https://arxiv.org/abs/1808.07452Tamara G. Kolda, David Hong. \"Stochastic Gradients for Large-Scale Tensor Decomposition\", SIAM Journal on Mathematics of Data Science 2:1066-1095, 2020. https://doi.org/10.1137/19M1266265 https://arxiv.org/abs/1906.01687","category":"page"},{"location":"","page":"Home","title":"Home","text":"In BibTeX form:","category":"page"},{"location":"","page":"Home","title":"Home","text":"@Article{hkd2020gcp,\n  title =        \"Generalized Canonical Polyadic Tensor Decomposition\",\n  author =       \"David Hong and Tamara G. Kolda and Jed A. Duersch\",\n  journal =      \"{SIAM} Review\",\n  year =         \"2020\",\n  volume =       \"62\",\n  number =       \"1\",\n  pages =        \"133--163\",\n  DOI =          \"10.1137/18M1203626\",\n}\n\n@Article{kh2020sgf,\n  title =        \"Stochastic Gradients for Large-Scale Tensor Decomposition\",\n  author =       \"Tamara G. Kolda and David Hong\",\n  journal =      \"{SIAM} Journal on Mathematics of Data Science\",\n  year =         \"2020\",\n  volume =       \"2\",\n  number =       \"4\",\n  pages =        \"1066--1095\",\n  DOI =          \"10.1137/19M1266265\",\n}","category":"page"},{"location":"#Docstrings","page":"Home","title":"Docstrings","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"","page":"Home","title":"Home","text":"Modules = [GCPDecompositions]\nFilter = t -> t !== ncomponents","category":"page"},{"location":"#GCPDecompositions.GCPDecompositions","page":"Home","title":"GCPDecompositions.GCPDecompositions","text":"Generalized CP Decomposition module. Provides approximate CP tensor decomposition with respect to general losses.\n\n\n\n\n\n","category":"module"},{"location":"#GCPDecompositions.CPD","page":"Home","title":"GCPDecompositions.CPD","text":"CPD\n\nTensor decomposition type for the canonical polyadic decompositions (CPD) of a tensor (i.e., a multi-dimensional array) A. This is the return type of gcp(_), the corresponding tensor decomposition function.\n\nIf M::CPD is the decomposition object, the weights Î» and the factor matrices U = (U[1],...,U[N]) can be obtained via M.Î» and M.U, such that A = Î£_j Î»[j] U[1][:,j] âˆ˜ â‹¯ âˆ˜ U[N][:,j].\n\n\n\n\n\n","category":"type"},{"location":"#GCPDecompositions.default_algorithm-Tuple{Array{var\"#s7\", N} where {var\"#s7\"<:Real, N}, Any, GCPDecompositions.GCPLosses.LeastSquaresLoss, Tuple{}}","page":"Home","title":"GCPDecompositions.default_algorithm","text":"default_algorithm(X, r, loss, constraints)\n\nReturn a default algorithm for the data tensor X, rank r, loss function loss, and tuple of constraints constraints.\n\nSee also: gcp.\n\n\n\n\n\n","category":"method"},{"location":"#GCPDecompositions.default_constraints-Tuple{Any}","page":"Home","title":"GCPDecompositions.default_constraints","text":"default_constraints(loss)\n\nReturn a default tuple of constraints for the loss function loss.\n\nSee also: gcp.\n\n\n\n\n\n","category":"method"},{"location":"#GCPDecompositions.default_init-NTuple{5, Any}","page":"Home","title":"GCPDecompositions.default_init","text":"default_init([rng=default_rng()], X, r, loss, constraints, algorithm)\n\nReturn a default initialization for the data tensor X, rank r, loss function loss, tuple of constraints constraints, and algorithm algorithm, using the random number generator rng if needed.\n\nSee also: gcp.\n\n\n\n\n\n","category":"method"},{"location":"#GCPDecompositions.gcp","page":"Home","title":"GCPDecompositions.gcp","text":"gcp(X::Array, r, loss = GCPLosses.LeastSquaresLoss();\n    constraints = default_constraints(loss),\n    algorithm = default_algorithm(X, r, loss, constraints),\n    init = default_init(X, r, loss, constraints, algorithm))\n\nCompute an approximate rank-r CP decomposition of the tensor X with respect to the loss function loss and return a CPD object.\n\nKeyword arguments:\n\nconstraints : a Tuple of constraints on the factor matrices U = (U[1],...,U[N]).\nalgorithm   : algorithm to use\n\nConventional CP corresponds to the default GCPLosses.LeastSquaresLoss() loss with the default of no constraints (i.e., constraints = ()).\n\nIf the LossFunctions.jl package is also loaded, loss can also be a loss function from that package. Check GCPDecompositions.LossFunctionsExt.SupportedLosses to see what losses are supported.\n\nSee also: CPD, GCPLosses, GCPConstraints, GCPAlgorithms.\n\n\n\n\n\n","category":"function"}]
}
